# ㅔㅛ쇄Pandas

### Series  - 1차원

0. Series의 기본

   ```python
   pd.Series(
   	data = None,   // 실제 데이터
     index = None,  // 데이터 인덱스 정보
     dtype = None,  // 타입지정
     name = None, 
     copy = False,
     fastpath = False,
   )
   ```

   ```python
   s = pd.Series(np.arange(5), np.arange(100, 105), dtype=np.int32)
   s1 = pd.Series(np.arange(5), s.index)
   ```

1. NaN 값 찾아내기

   ```python
   s = pd.Series([1, 2, 3, 4, 5, np.NaN]) 
   s.size - s.count()   // s.size=개수, s.count()=NaN제외개수 
   ```

2. 평균과 빈도수

   ```python
   s.mean()  // numpy의 경우는 NaN값이 있으면, 평균값을 반환하지 못함. (Series는 가능)
   s.value_counts()   // (제일 중요) NaN 제외 빈도수 알 수 있음.
   ```

3. 미리보기

   ```python
   s.head()
   s.tail()
   s.unique()   // 유일한 값만 ndarray로 반환
   s.shape    // 튜플로 shape 반환
   ```

4. 연산

   : 기본적으로 같은 인덱스끼리 연산이 된다. 만약, pair 맞지 않으면, NaN 반환.

   ```python
   s[s>5]       // 값이 5보다 큰것 반환
   s[s.index>5] // 인덱스가 5보다 큰것 반환
   s[s%2==0]    // 값이 짝수만 반환
   s[(s>5) & (s<8)]  // 멀티플
   ```

5. 인덱싱

   ```python
   s[[1, 5, 6]]
   s[1:3]        // 인덱트가 숫자, 문자 상관없이 사용가능
   s['a':'b']    // 인덱스가 문자인 경우 사용(마지막 문자 포함되어 반환됨)
   ```

6. inplace

   : 파라미터 중에 inplace는 해당 메서드를 원본에 해버릴지를 묻는 것임.

   ```python
   s.drop('k')   // 인덱스가 k인 행 없앰. (원본에 영향 없음)
   s.drop('k', inplace = True)  // 원본에 영향 ㅇ
   ```



### DataFrame - 2차원

1. 파일 읽기

   - sep - 구분자 설정
   - header - 읽을 때, 변수 세팅 있으면, True(Default)
   - index_col - 인덱스로 쓸 변수 있으면 지정
   - usecols - 로딩할 때, 필요한 columns만 가져올 수 있음.

   ```python
   data = pd.read_csv('./sample.csv')   // 기본
   data = pd.read_csv('./sample.csv', index_col='ID', usecols=['ID', 'Name', 'Age'])
   ```

2. 확인하기

   ```python
   data.head()
   data.tail()
   data.shape()     // (r, c) 반환
   data.describe()  // 숫자형 데이터 통계치 반환
   data.info()      // 데이터 타입 반환
   data.index       // 데이터 인덱스 정보
   data.columns     // 변수 확인
   ```

3. 생성하기

   ```python
   // 1. dictionary로 만들기
   data = {'a' : [1, 2, 3], 'b' : [4, 5, 6], 'c' : [10, 11, 12]}
   pd.DataFrame(data, index=[0, 1, 2])
   
   // 2. Series로 만들기
   a = pd.Series([100, 200, 300], ['a', 'b', 'c'])
   b = pd.Series([101, 201, 301], ['a', 'b', 'c'])
   c = pd.Series([110, 210, 310], ['a', 'b', 'c'])
   pd.DataFrame([a, b, c], index=[100, 101, 102])
   ```

4. 원하는 col, row 

   ##### 1) col 접근

   : numpy와 달리, [] 안에 값이 하나만 있으면, 이것은 col 단위로 인식한다.

   ```python
   data['Name']          // 하나의 column
   data[['Name', 'Age']] // 두개 이상의 columns
   ```

   

   ##### 2) row 접근

   ```python
   data[:10]    // 슬라이싱
   ```

   - loc - (내가 지정한) 인덱스 사용

   - iloc - (컴퓨터가 보이는) 인덱스 사용

     예시)

     ![image](https://user-images.githubusercontent.com/42775225/75620706-69d2c500-5bcf-11ea-96a2-9fa5ada5ef94.png)

   ```python
   data.loc['a']
   data.iloc[0]
   
   => 둘의 결과가 같다.
   ```

   

   ##### 3) row, col 동시 접근

   ![image](https://user-images.githubusercontent.com/42775225/75620748-f9787380-5bcf-11ea-8f41-2cf7774b8354.png)



5. 







# 참고자료

1. 데이터 사이언스 스쿨

   : https://datascienceschool.net/view-notebook/704731b41f794b8ea00768f5b0904512/

2. 금융데이터 위키

   : https://wikidocs.net/47237











# Spark

 : 실시간 분산형 컴퓨팅 프로젝트로, 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델이다. 컴퓨터 클러스터로 여러 컴퓨터의 자원을 모아서 처리할 때, 작업을 조율할 수 있도록 해주는 프레임워크이다.

​	1) 구조적 API : 

​	ex) Dataset, DataFrame, SQL

​	2) 저수준 API : 2가지의 operation을 가진다. transformation, action. 해당 데이터 프레임이나 자신의 원본 데이터프레임에 액션이 호출되기 전까지 데이터를 읽지 않는다.

​	ex) RDD, 분산형 변수

​	3) 구조적 스트리밍, 고급분석, 라이브러리 및 에코 시스템



## 구조적 API

#### [ 스파크 코드가 클러스터에서 실제 처리되는 과정 ]

1) 사용자 코드를 논리적 실행계획으로 변환한다. 

2) 논리적 실행게획을 물리적 실행계획으로 변환하며, 그 과정에서 카탈로그 옵티마이저가 추가적인 최적화를 할 수 있는지 확인한다.

3) 물리적 실행계획(RDD처리)를 실행한다.





1) Dataset

 :  JVM 기반 언어인, 자바와 스칼라의 정적 데이터 타입에 맞는 코드이다. 파이썬과 R에서는 사용할 수 없다. (타입형으로 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인한다.)

2) DataFrame

 : 다양한 데이터 타입의 테이블형 데이터를 보관할 수 있는 Row타입의 객체로 구성된 분산 컬렉션이다. (비타입형으로 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서 확인한다.)

cf) 데이터 타입

 : 스파크 데이터 타입을 파이썬에서 사용하려면 다음과 같은 코드를 사용한다.

```python
from pyspark.sql.types import *
b = ByteType()
```



## 구조적 스트리밍

 : 스트림 처리용 고수준API로서, 구조적 API로 개발된 배치 모드의 연산을 스트리밍 방식으로 실행할 수 있으며, 지연 시간을 줄이고 증분 처리할 수 있다.

1) 스트리밍 유형의 DataFrame 생성

```
streamingDataFrame = spark.readStream
		.scheme(s)
```





## 환경

 : 로컬에서 사용하려면, 자바가 설치되어 있어야하며, 스파크를 내려받아야 한다. 

```bash
pip install pyspark
```

1) 파이썬 콘솔 사용하기 (혹은)

```bash
./bin/pyspark
```

1) 대화형 세션 사용하기

```bash
./bin/spark-shell    // 이렇게 하면, SparkSession이 자동으로 생성
```



2) SparkSession 출력하기

```bash
spark
```

3) 스탠드얼론 어플리케이션을 스파크에 제출하기

```bash
spark-submit
```



## 파일 읽기

1) csv파일 읽기

```python
# 1.
flights = spark
		.read
		.option("inferSchema", "true")
    .option("header", "true")
    .csv("/data/flight-data/flight.csv")

# 2.
# spark.createDataFrame() : 스파크 라이브러리에서 제공하는 함수, 최적화된 데이터프레임 생성
# pd.read_csv() : 외부 text 파일, csv 파일을 불러와 데이터프레임으로 저장
file_url = 'https://github.com/hbchoi/BigDataRep/raw/master/DallasCouncilVoters.csv.gz'
df = spark.createDataFrame(pd.read_csv(file_url, compression='gzip', error_bad_lines=False))

# 3.
# sqlContext.read.text() : 텍스트 파일을 데이터프레임으로 변환해주는 함수
url = 'https://github.com/hbchoi/BigDataRep/raw/master/shakespeare.txt'
shakespeare_raw_DF = spark.createDataFrame(pd.read_csv(url, error_bad_lines=False, header = None, sep = '#', names = ['value'], skip_blank_lines=False).fillna(''))

```

2) 로그 데이터의 경우 --> regexp_extract함수를 이용하자

![image](https://user-images.githubusercontent.com/42775225/78755393-0862fa80-79b4-11ea-9525-2a609809e740.png)

![image](https://user-images.githubusercontent.com/42775225/78755424-19ac0700-79b4-11ea-9516-32289e9ed0ab.png)



#### [중요]

 : 기본적으로 데이터 받아오면, null값 있는지부터 확인하자

```python
base_df.filter(base_df['value'].isNull()).count()
```

```python
bad_rows_df = split_df.filter(split_df['host'].isNull() |
                              split_df['timestamp'].isNull() |
                              split_df['path'].isNull() |
                              split_df['status'].isNull() |
                             split_df['content_size'].isNull())
bad_rows_df.count()
```

 : 이렇게 확인했는데, null 값이 있다면, 어느 column에 그런 null 값이 있는지를 확인해본다.

```python
from pyspark.sql.functions import col, sum

def count_null(col_name):
  return sum(col(col_name).isNull().cast('integer')).alias(col_name)

# na.fill을 이용해서 적당한 값을 넣어주고 다시 확인한다.
cleaned_df = split_df.na.fill({'content_size': 0})

exprs = []
for col_name in split_df.columns:
  exprs.append(count_null(col_name))

# Run the aggregation. The *exprs converts the list of expressions into
# variable function arguments.
split_df.agg(*exprs).show()
```



### timestamp parsing하기

![image](https://user-images.githubusercontent.com/42775225/79098579-42117800-7d9d-11ea-9581-865450306c6b.png)

```python
def parse_clf_time(s):
    """ Convert Common Log time format into a Python datetime object
    Args:
        s (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]
    Returns:
        a string suitable for passing to CAST('timestamp')
    """
    # NOTE: We're ignoring time zone here. In a production application, you'd want to handle that.
    return "{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}".format(
      int(s[7:11]),
      month_map[s[3:6]],
      int(s[0:2]),
      int(s[12:14]),
      int(s[15:17]),
      int(s[18:20])
    )

u_parse_time = udf(parse_clf_time)

logs_df = cleaned_df.select('*', u_parse_time(cleaned_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp')
total_log_entries = logs_df.count()

```





## SQL

 : 스파크는 SQL 쿼리를 사용하려면, DataFrame을 테이블이나 뷰로 등록해야 쓸 수 있다.

```python
df.createOrReplaceTempView("flight_data")
```







## RDD

 : 불변성을 가지며, 병렬로 처리할 수 있는 파티셔닝된 레코드(객체)의 모음입니다. 모든 DataFrame이나 Dataset 코드는 RDD로 컴파일 됩니다. 물리적으로 분산된 데이터에 세부적인 제어가 필요할 때 RDD를 사용하는 것이 가장 적합합니다.

1) RDD 생성

```python
from pyspark.sql import Row
spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()
```





1) distinct

2) filter    

​	ex) words.filter(lambda word: startsWithS(word)).collect()

3) map : 하나의 RDD를 받아서 하나의 RDD로 바꾸는것

4) flatMap : 하나의 RDD를 여러 RDD 결과로 변환하고 싶을 때 사용.

4) sortBy

​	ex) words.sortBy(lambda word: len(word) * -1).take(2)

5) reduce

6) count





## sqlContext

 : SparkSession안의 개념으로, RDD와 같은 API 사용가능. 



## SparkConf 

 : 객체는 사용자가 재정의해서 쓸 수 있는 **설정 옵션**들에 대한 키와 값 쌍들이 존재

```python
spark.conf.set("spark.sql.shuffle.partitions", 5)
spark.conf.get('spark.app.name')
spark.conf.get('spark.driver.port')
spark.conf.get('spark.sql.shuffle.partitions')
```



## cluster

 :



## driver = SparkSession

 : 클러스터 노드 중 하나에서 실행되며, main() 함수를 실행합니다. executor 프로세스의 작업과 관련된 분석, 배포 스케줄링 역할을 수행함.



## excutor

 : 드라이버가 할당한 작업을 수행하는 것. RDD가 위치하는 공간.



## Partitioning

 : 모든 excutor가 병렬로 작업을 수행할 수 있도록 파티션이라 불리는 청크 단위로 데이터를 분할한다. 어떤 데이터가 어느 노드에서 끝날지를 결정하는 것.











## transformation

 : 스파크의 핵심 데이터 구조가 불변성이다. 그런데, 변경하려면 원하는 변경 방법을 스파크에게 알려줘야하는데 이것이 transformation이다. RDD에서 새로운 RDD를 생성하는 함수 

​	1) narrow dependency : 하나의 입력 파티션이 하나의 출력 파티션에만 영향 ex) filter

​	2) wide dependency : 하나의 입력 파티션이 여러 출력 파티션에 영향 ex) count

ex) map, filter, flatMap, join, sum, withColumnRenamed, count 등



## action

 : 지정된 트랜스포메이션 연산을 시작하려면 액션을 사용한다. 이것이 실제 연산이다. 데이터를 드라이버로 모으거나 외부 데이터 소스로 내보낼 수 있다. RDD에서 RDD가 아닌 타입의 data 로 변환하는 함수들.하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG(지향성 비순환 그래프) 처리 프로세스를 실행한다.

ex) count, collect, reduce, save 등



## 예제

![image](https://user-images.githubusercontent.com/42775225/78747841-5d4b4480-79a5-11ea-90ee-7695bfad8685.png)

![image](https://user-images.githubusercontent.com/42775225/78747861-6dfbba80-79a5-11ea-8bed-e113ab223a06.png)





## lazy evaluation

 : 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미한다. 불필요한 연산을 피하도록, 필요연산 전까지 대기 하는것.



## join

 : 서로 다른 두 표를 결합하는 것.

​	1) 내부 조인(inner join) : 왼쪽과 오른쪽 데이터 셋에 키가 있는 로우를 유지

​	2) 외부 조인(outer join) : 왼쪽이나 오른쪽 데이터 셋에 키가 있는 로우를 유지

​	3) 왼쪽 외부 조인(left outer join) : 왼쪽 데이터 셋에 키가 있는 로우를 유지

​	4) 오른쪽 외부 조인(right outer join) : 오른쪽 데이터 셋에 키가 있는 로우를 유지

​	5) 왼쪽 세미 조인(left semi join) : 왼쪽 데이터 셋의 키가 오른쪽 데이터 셋에 있는 경우에는 키가 일치하는 왼쪽 데이터 셋만 유지

​	6) 왼쪽 안티 조인(left anti join) : 왼쪽 데이터 셋의 키가 오른쪽 데이터 셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터 셋만 유지

​	7) 자연 조인(natural join) : 두 데이터 셋에서 동일한 이름을 가진 컬럼을 암시적으로 결합하는 조인을 수행

​	8) 교차 조인(cross join) | 카테시안 조인(Cartesian join) : 왼쪽 데이터 셋의 모든 로우와 오른쪽 데이터 셋의 모든 로우를 조합





## 집계연산

 : 파이썬에서는 집계함수를 다음 패키지로 사용할 수 있다.

```python
from pyspark.sql.functions import *s
```

#### [집계 함수]

1) count

2) countDistinct

3) approx_count_distinct

4) first와 last

5) min과 max

6) sum

7) sumDistinct

8) avg

9) 분산과 표준편차

10) 비대칭도와 첨도

11) 공분산과 상관관계

12) 복합 데이터 타입의 집계



## Pyspark Basic 함수들

0) createDataFrame(param)

 : param으로 [(행, 열)], [행이름, 열이름]을 넘겨주며, 데이터프레임을 만드는 함수이다.

```python
df = spark.createDataFrame([('cat',), ('elephant',), ('rat',), ('rat',), ('cat', )], ['word'])
```

## 



1) groupBy(param)

  : param으로 column을 넘겨주며, 해당 column을 기준으로 데이터를 그룹화할 수 있으며, 집계 함수(min, max, count, avg, sum 등)를 사용할 수 있다.

```python
fligths.groupBy('month')
flights.groupBy('month', 'dest')
flights.filter(flights.origin == 'PDX').groupBy().max('distance').show()
flights.withColumn("duration_hrs", flights.air_time/60).groupBy().sum("duration_hrs").show()
```



2) orderBy(param) 

  : param으로 column을 넘겨주며, 해당 column을 기준으로 데이터를 정렬할 수 있다. default는 오름 차순이다. (파이썬에 내장되어 있는 sort함수를 사용할 수도 있다.)

```python
df.orderBy('Sales').show()
df.orderBy(df['Sales'].desc()).show()
df.orderBy(df.Name.desc()).show(10)

df.sort('Sales').take(2)
df.sort(desc('Sales')).take(2)
```



3) select(param)

 : param으로 column을 넘겨주며, 해당 column을 추출한다. 여러 개의 column 가능. 그리고, column 값에 연산한 결과를 보고싶을 때도 사용할 수 있다.

```python
flights.select("tailnum", "origin", "dest")
flights.select((flights.air_time/60).alias("duration_hrs")).show()
```



4) selectExpr(param)

 : param으로 'column명 as 별명' 을 넘겨주며, .alias() 메서드의 역할을 해주는 select함수이다.

```python
flights.selectExpr("air_time/60 as duration_hrs")
```

 : 아니면 그냥 바로 써도 된다.

```python
content_stats = (parsed_points_df
                 .selectExpr("min(label)", "max(label)")).collect()
```



5) collect()

 : 모든 row를 반환한다.



5-1) sellect(param)

 ; param으로c column을 넘겨주어 column을 반환한다.



6) count()

 : action으로 사용될 수도 있고, transformation으로 사용될 수도 있다.

​	[1] action의 경우

```python
df.count()   
df.groupBy("Sales").count()   # 해당 Sales의 종류가 몇개가 있는지 count라는 새로운 column으로 반환 
```

​	[2] transformation의 경우

```python
import pyspark.sql.functions import count
df.select(count('ID')).show()
```



7) dir(param), help(param)

 : param으로 object를 넘겨주며, 해당 object의 attributes(methods)의 리스트를 얻을 수 있는 파이썬의 내장 함수이다. 

```python
dir(sqlContext)
help(sqlContext)
```



8) agg(param)

 : param으로 집계함수를 넘겨주며, 여러 집계 처리를 한 번에 지정할 수 있다.

```python
from pyspark.sql.functions import count

df.groupBy("origin").agg(
	count("quality").alias("quan"),
	expr("count(quality)")).show()
```



9) expr(param)

 : param으로 컬럼이나 문자열을 넘겨주며, 컬럼을 참조하는 함수이다. select함수와 함께 많이 사용한다.

```python
df.select(expr("DEST_COUNTRY AS destiantion")).show()
df.withColumn("destination", expr("DEST_COUNTRY")).columns
```



10) lit(param)

 : param으로 object(integer, string)을 넘겨주며, 스파크 데이터 타입으로 변환해준다.

```python
from pyspark.sql.functions import lit
df.select(lit(5), lit("five"), lit(5.0))

from pyspark.sql.functions import lit, concat
df.select(concat(df['value'], lit('*'))).show(truncate=False)
```



11) cache()

 : 스파크는 최초 연산 시 데이터를 메모리나 디스크에 저장한다. 그런 다음 캐싱된 데이터프레임을 사용하는 쿼리를 수행하면 원본 파일을 읽는 대신 메모리에 저장된 데이터를 참조한다. (보다 정교한 제어를 위해 사용하는 persist메서드는 데이터 캐시영역을 지정하는 StorageLevel객체를 파라미터로 사용한다.)

```python
df.cache()
df.is_cached
df.unpersist()  # cache 제거
```



12) describe(param)

 : param으로 column을 넘겨주며, count, mean, stddev, min, max 값을 리턴해준다.

```python
logs_df.describe(['content_size']).show()		
```



13) display(param)

 : param으로 데이터 프레임을 넘겨주며, bar chart를 리턴한다.



14) show(param)

 : param으로 데이터 프레임을 넘겨주며, truncate는 짧게 할 것인지를 결정한다. 가령, False이면, 생략없이 다 보여준다. 숫자를 지정하면, 값의 길이를 해당 숫자만큼으로 제한한다.



15) map(param)

 : param으로 lambda 함수를 넘겨주며, 주어진 입력을 원하는 값으로 반환하는 함수(lambda함수)를 명시하고, 레코드별로 적용한다.

```python
days_with_hosts =daily_hosts_df.rdd.map(lambda r: r[0]).collect()
hosts =daily_hosts_df.rdd.map(lambda r:r[1]).collect()
```



16) sort(param)

 : param으로 column을 넘겨주며, ascending 조건도 줄 수 있다.

```python
df.groupBy('path')
  .count()
  .sort('count', ascending=False).show()
```



17) select(param)

 : param으로 column 넘겨주며, 해당 column을 반환한다.

```python
df.select('host').distinct().count()
```



18) distinct()

 : row를 기준으로 중복 데이터를 제거한다.

```python
df.distinct()
df.select("name").distinct()
```



19) pyspark.sql.functions

```python
import pyspark.sql.functions as F
from pyspark.sql.functions import when

# F.split : 열을 특정 문자 기준으로 분할 (반환값 : array type)
df = df.withColumn('separated', F.split(df.name, '\s+'))  # \s+ : 공백문자

# getItem : array type의 n번째 값을 가져온다
df = df.withColumn('first', df.name.getItem(0))

# F.size : array type 열의 크기를 개산해준다.
df = df.withColumn("last", df.separated.getItem(F.size('separated') - 1))

# contains() : 특정 문자열의 포함 여부를 반환해주는 함수
df = df.filter(~ F.col('name').contains("_"))

# F.when(if, then) : 데이터프레임을 조건부로 수정해주는 함수
# F.rand() : 0~1 사이의 랜덤값 반환
# otherwise() : if ~ else와 동일
df = df.withColumn('random_val',
                     when(df.title == 'Tom', F.rand())
                     .when(df.title == 'Mayor', 2)
                     .otherwise(0))

# F.udf() : 메소드 체인에서 직접 사용할 수 있도록 변환해주는 함수
def getFirstAndMiddle(names):
  return ' '.join(names[:-1])
  # [::-1] : 뒤집어서 슬라이스 -> 배열
  # [::1] : 그대로 슬라이스 -> 배열
udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())

# F.monotonically_increasing_id() : 순차적으로 id값 부여
df = df.withColumn('ROW_ID', F.monotonically_increasing_id())

# repartition() : 파티션 수를 늘리거나 줄이는 것 모두 가능
df = df.repartition(8)

# coalesce() : 파티션의 크기를 나타내는 정수를 인자로 받아서 파티션의 수 조정(줄이는 것만 가능)??
df = df.coalesce(1)

# concat(A, B, ..) : 여러 입력 열을 단일 열로 연결해주는 함수(문자열, 이진 등)
# lit() : 데이터프레임에 없고 내가 원하는 값만 들어가는 column을 생성하고 싶을 때 추가 함수
df = df.select(concat(df.word, lit('s')).alias('word'))

# length(col) : 열에 있는 각 개체의 길이 반환
df = df.select(length('word').alias('length(word)'))

# regexp_replace(str, pattern, replacement) : substr()과 동일
# trim(col) : String 양쪽 끝의 공백을 제거해주는 함수
# lower(col) : String을 소문자로 변환해주는 함수
# \s : 공백(whitespace, tab, ...)문자
# ^ : start of line
# [^\w\n]+ : 개행 단어를 제외한 1개 이상의 비 단어 문자(non-[a-zA-Z0-9_])
# | : or
# [^\w\n]+$ : 줄 끝에서 1개 이상의 비 단어 문자($)
# [^\w\s]+ : 공백을 제외한 1개 이상의 비 단어 문자
# _ : underscore
def removePunctuation(column):
    return trim(regexp_replace(
      lower(column), '^[^\w\n]+|[^\w\n]+$|[^\w\s]+|_', '')).alias('sentence')

# explode(col) : 하나의 row를 여러 개로 분리해주는 함수
# where(조건) : 조건에 맞는 데이터프레임을 반환해주는 함수
shakeWordsDF = (shakespeareDF
                .where(trim(shakespeareDF.sentence) != "")
                .select(explode(split('sentence',"\s+"))
                        .alias('word')))
```

cf) explode, where로 한 것.

![image](https://user-images.githubusercontent.com/42775225/79072040-d803bf00-7d19-11ea-9085-7784faef34e7.png)    -->.     ![image](https://user-images.githubusercontent.com/42775225/79072046-de923680-7d19-11ea-9149-2e068ac15877.png)



20) drop(param)

 : param으로 지우고 싶은 column을 넘겨준다.

```python
df = df.drop('first')
```

 : null값 제거할 때도 사용한다.





21) df.printScheme()

 : 데이터프레임의 스키마를 알 수 있다.

```python
df.printScheme()
```



22) df['column'].rlike(param)

 : param으로, 정규표현식을 넘겨준다. 정규표현식에 맞는 value를 담고 있는 것들만 추출하게 된다.

```python
bad_content_size_df = base_df.filter(~ base_df['value'].rlike(r'\d+$')) # ~ means 'not'
```



23) (중요) df.na.fill(param)

 : param으로, column과 값을 넘겨준다. 빈 값(NULL)에 값을 넣을 수 있다.

```python
cleaned_df = split_df.na.fill({'content_size': 0})
```



24) log(param)

 : parma으로 column을 넘겨준다. 데이터 값들이 불균형할 때 사용한다.

```python
log_status_to_count_df = status_to_count_df.withColumn('log(count)', sqlFunctions.log(status_to_count_df['count']))
```



25) df.dropDuplicates()

 : 속성(열)값이 같은 중복 행 삭제한다.

```python
unique_hosts_df = host_df.dropDuplicates()
```



26) reduce(param)

 : param으로 function을 넘긴다. RDD의 모든 값을 하나의 값으로 만든다.

```python
spark.sparkContext.parallelize(range(1, 21)).reduce(lambda x, y:x+y)  # 값은 210
```



27) coalesce(param)

 : param으로 column을 넘겨주며, 인수로 지정한 컬럼 중 null이 아닌 첫번 째 값을 반환한다. 모든컬럼이 null이 아니면, 첫번 째 컬럼의 값을 반환한다.

```python
from pyspark.sql.functions import coalesce
df.select(coalesce(col("Description"), col("CustomerId"))).show()
```



28) explode(param)

 : param으로 배열타입의 컬럼을 넘겨주며, 배열 값의 모든 값을 row로 변환한다. 나머지 컬럼 값은 중복되어 표시된다.

![image](https://user-images.githubusercontent.com/42775225/82138669-2cd8af00-985d-11ea-8081-d8236617e047.png)

```python
from pyspark.sql.functions import split, explode

df.withColumn("splitted", split(col("Description"), " "))\
  .withColumn("exploded", explode(col("splitted")))\
  .select("Description", "InvoiceNo", "exploded").show(2)
```

<img src="https://user-images.githubusercontent.com/42775225/82138761-e20b6700-985d-11ea-9aee-ba450d64f826.png" alt="image" style="zoom:50%;" />





## One-hot-encoding 

 : 'hello world' 를 벡터 값으로 바꿔보자.

```python
data = 'hello world'

alphabet = 'abcdefghijklmnopqrstuvwxyz '

char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))

integer_encoded = [char_to_int[char] for char in data]  # 이 스킬을 알아두자.
print(integer_encoded)
```









## Null 값 다루기

1) drop

```python
df.na.drop()        # 
df.na.drop("any")   # 컬럼값 중 하나라도 null 값을 가지면 해당 로우 제거
df.na.drop("all")   # 모든 column의 값이 null이거나 NaN인 경우에만 해당 로우 제거
df.na.drop("all", subset=["StockCode", "InvoiceNo"])
```



2) fill

 : 데이터 타입 별로, 값을 특정 값으로 채울 수 있다.

```python
df.na.fill("pass")    # String데이터 타입에 존재하는 null값을 "pass"라는 string으로 대체
df.na.fill(5:Integer) # Integer데이터 타입에 존재하는 null값을 5로 대체
df.na.fill(5:Double)  # Double데이터 타입에 존재하는 null값을 5로 대체
df.na.fill("all", subset=["StockCode", "InvoiceNo"]) # 다수의 컬럼에 적용

fill_cols_vals = {"StockCode":5, "Description": "No Value"}
df.na.fill(fill_cols_vals)   # Map타입으로 key(컬럼명), value(대체할 값)
```



3) replace

 : 이건 조건에 따라 다른 값으로 대체하는 것임. (변경하고자 하는 값과 원래 값의 데이터 타입이 같아야 한다.)

```python
df.na.replace([""], ["UNKNOWN"], "Description")
```



## 예제

1) 수식에 맞게 코드 짜보기

![image](https://user-images.githubusercontent.com/42775225/80863186-81085e80-8cb5-11ea-8257-0fdf419dde80.png)

여기셔의 첫번째 연산에 해당하는 부분

```python
def gradient_summand(weights, lp):
    return (weights.dot(lp.features) -  lp.label) * DenseVector(lp.features)
  
def get_labeled_prediction(weights, observation):
    return float(weights.dot(observation.features)), float(observation.label)
  
weights = np.array([1.0, 1.5])
prediction_example = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),
                                     LabeledPoint(1.5, np.array([.5, .5]))])

preds_and_labels = prediction_example.map(lambda lp: get_labeled_prediction(weights, lp))
```





# 파이썬

1) zip(param)

 : param으로 반복 가능한 자료형 여러개를 넘겨줄 수 있다.

```python
list(zip([1, 2, 3], [4, 5, 6], [7, 8, 9]))

=> [(1, 4, 7), (2, 5, 8), (3, 6, 9)]
```





## import re

 : 정규표현식 모듈로서, 특정한 패턴과 일치하는 문자열를 '검색', '치환', '제거' 하는 기능을 지원한다.

### [ 기본개념 ]

1. #### raw string

   : ' ' 앞에 r을 붙이는 것. 문자열 앞에 r이 붙으면 해당 문자열이 구성된 그대로 문자열로 변환한다.

   

#### 2. 패턴에 대하여

##### 1. 기본

- a, X, 9 등등 문자 하나하나의 character들은 정확히 해당 문자와 일치
  - e.g) 패턴 test는 test 문자열과 일치
  - 대소문자의 경우 기본적으로 구별하나, 구별하지 않도록 설정 가능
- 몇몇 문자들에 대해서는 예외가 존재하는데, 이들은 틀별한 의미로 사용 됨
  - . ^ $ * + ? { } [ ] \ | ( )
- . (마침표) - 어떤 한개의 character와 일치 (newline(엔터) 제외)
- \w - 문자 character와 일치 [a-zA-Z0-9_]
- \s - 공백문자와 일치
- \t, \n, \r - tab, newline, return
- \d - 숫자 character와 일치 [0-9]
- ^ = 시작, $ = 끝 각각 문자열의 시작과 끝을 의미
- \가 붙으면 스페셜한 의미가 없어짐. 예를들어 \.는 .자체를 의미 \\는 \를 의미
- 자세한 내용은 링크 참조 https://docs.python.org/3/library/re.html

##### 2. []

- [] 내부의 메타 캐릭터는 캐릭터 자체를 나타냄

- [abck] : a or b or c or k
- [abc.^] : a or b or c or . or ^
- [a-d] : -와 함께 사용되면 해당 문자 사이의 범위에 속하는 문자 중 하나
- [0-9] : 모든 숫자
- [a-z] : 모든 소문자
- [A-Z] : 모든 대문자
- [a-zA-Z0-9] : 모든 알파벳 문자 및 숫자
- [^0-9] : ^가 맨 앞에 사용 되는 경우, 해당 문자 패턴이 아닌 것과 매칭

##### 3. \

1. 다른 문자와 함께 사용되어 특수한 의미를 지님

- \d : 숫자를 [0-9]와 동일

- \D : 숫자가 아닌 문자 [^0-9]와 동일

- \s : 공백 문자(띄어쓰기, 탭, 엔터 등)

- \S : 공백이 아닌 문자

- \w : 알파벳대소문자, 숫자 [0-9a-zA-Z]와 동일

- \W : non alpha-numeric 문자 [^0-9a-zA-Z]와 동일

2. 메타 캐릭터가 캐릭터 자체를 표현하도록 할 경우 사용

- ```
  \. , \\
  ```

##### 4. **반복패턴**

- 패턴 뒤에 위치하는 *, +, ?는 해당 패턴이 반복적으로 존재하는지 검사
  - '+' -> 1번 이상의 패턴이 발생
  - '*' -> 0번 이상의 패턴이 발생
  - '?' -> 0 혹은 1번의 패턴이 발생
- 반복을 패턴의 경우 greedy하게 검색 함, 즉 가능한 많은 부분이 매칭되도록 함
  - e.g) a[bcd]*b 패턴을 abcbdccb에서 검색하는 경우
    - ab, abcb, abcbdccb 전부 가능 하지만 최대한 많은 부분이 매칭된 abcbdccb가 검색된 패턴

##### 5. **^**, **$**

- ^ 문자열의 맨 앞부터 일치하는 경우 검색
- $ 문자열의 맨 뒤부터 일치하는 경우 검색

##### 6. **grouping**

- ()을 사용하여 그루핑

- 매칭 결과를 각 그룹별로 분리 가능

- 패턴 명시 할 때, 각 그룹을 괄호() 안에 넣어 분리하여 사용

  ```python
  m = re.search(r'(\w+)@(.+)', 'test@gmail.com')
  print(m.group(1))   # 첫번째 ()
  print(m.group(2))   # 두번째 ()
  print(m.group(0))   # 0번은 기본값
  ```

##### **7. {}**

- *, +, ?을 사용하여 반복적인 패턴을 찾는 것이 가능하나, 반복의 횟수 제한은 불가

- 패턴뒤에 위치하는 중괄호{}에 숫자를 명시하면 해당 숫자 만큼의 반복인 경우에만 매칭

- {4} - 4번 반복

- {3,4} - 3 ~ 4번 반복

  ```python
  re.search('pi{3,5}g', 'piiiiig')
  ```

##### **8. 미니멈 매칭(non-greedy way)**

- 기본적으로 *, +, ?를 사용하면 greedy(맥시멈 매칭)하게 동작함

- *?, +?을 이용하여 해당 기능을 구현

  ```python
  re.search(r'<.+>', '<html>haha</html>')
  ```

##### **9. {}?**

- {m,n}의 경우 m번 에서 n번 반복하나 greedy하게 동작

- {m,n}?로 사용하면 non-greedy하게 동작. 즉, 최소 m번만 매칭하면 만족

  ```python
  re.search(r'a{3,5}', 'aaaaa')
  re.search(r'a{3,5}?', 'aaaaa')
  ```

##### 10. **match**

- search와 유사하나, 주어진 문자열의 시작부터 비교하여 패턴이 있는지 확인

- 시작부터 해당 패턴이 존재하지 않다면 None 반환

  ```python
  re.match(r'\d\d\d', 'my number is 123')
  re.match(r'\d\d\d', '123 is my number')
  re.search(r'^\d\d\d', '123 is my number')
  ```

##### 11. **findall**

- search가 최초로 매칭되는 패턴만 반환한다면, findall은 매칭되는 전체의 패턴을 반환

- 매칭되는 모든 결과를 리스트 형태로 반환

  ```python
  re.findall(r'[\w-]+@[\w.]+', 'test@gmail.com haha test2@gmail.com nice test test')
  ```

##### 12. **sub**

- 주어진 문자열에서 일치하는 모든 패턴을 replace

- 그 결과를 문자열로 다시 반환함

- 두번째 인자는 특정 문자열이 될 수도 있고, 함수가 될 수 도 있음

- count가 0인 경우는 전체를, 1이상이면 해당 숫자만큼 치환 됨

  ```python
  re.sub(r'[\w-]+@[\w.]+', 'great', 'test@gmail.com haha test2@gmail.com nice test test', count=1)
  ```

##### 13. **compile**

- 동일한 정규표현식을 매번 다시 쓰기 번거로움을 해결

- compile로 해당표현식을 re.RegexObject 객체로 저장하여 사용가능

  ```python
  email_reg = re.compile(r'[\w-]+@[\w.]+')
  email_reg.search('test@gmail.com haha good')
  email_reg.findall()
  ```

##### 14. 나머지

| 특수 기호   | 의미                              | 예                                           |
| ----------- | --------------------------------- | -------------------------------------------- |
| `.`         | 개행 문자를 제외한 아무 문자 하나 | `파..`: `파이썬`, `파랑새`, `파김치`와 매치  |
| `^`         | 텍스트의 시작지점                 | `^a`: `ab`와 매치, `ba`와는 매치하지 않음    |
| `$`         | 텍스트의 종료지점                 | `a$`: `ba`와 매치 `ab`와는 매치하지 않음     |
| `+`         | 앞의 문자가 1번 이상 등장         | `ab+`: `ab`, `ab`, `abbbb` 등과 매치         |
| `?`         | 앞의 문자가 0번 또는 1번만 등장   | `ab?`: `a`, `ab`와 매치                      |
| `*`         | 앞의 문자가 0번 이상 등장         | `ab*`: `a`, `ab`, `abbb` 등과 매치           |
| `{n}`       | 앞의 문자가 `n`번 등장            | `ab{3}`: `abbb`와 매치                       |
| `{m,n}`     | 앞의 문자가 `m` - `n`번 등장      | `ab{1,3}`: `ab`, `abb`, `abbb`와 매치        |
| `(a|b)`     | `a` 또는 `b`                      | `a(b|c)`: `ab`, `ac`와 매치                  |
| `[문자들]`  | 대괄호 속의 문자 중 하나          | `a[bcd]`: `ab`, `ac`, `ad`와 매치            |
| `[^문자들]` | 대괄호 속의 문자가 아닌 문자 하나 | `a[^bcd]`: `aa`, `ae`, `af`등과 매치         |
| `[0-9]`     | `0`, `9` 사이의 모든 문자         | `[0-9]+`: `0`, `1234` 등과 매치              |
| `[A-Z]`     | `A`, `Z` 사이의 모든 문자         | `[A-Z]+`: `I`, `PYTHON` 등과 매치            |
| `[a-z]`     | `a`, `z` 사이의 모든 문자         | `[A-Z][a-z]+`: `Aa`, `Python` 등과 매치      |
| `[가-힣]`   | `가`, `힣` 사이의 모든 문자       | `[가-힣]+`: `파이썬`, `프로그래밍` 등과 매치 |

| 특수 기호 | 의미                              | 비슷한 표현          |
| --------- | --------------------------------- | -------------------- |
| `\\`      | 백슬래시(`\`)                     |                      |
| `\d`      | 모든 숫자                         | `[0-9]`              |
| `\D`      | 숫자 외의 문자                    | `[^0-9]`             |
| `\s`      | 공백 문자                         | `[ \t\n\r\f\v]`      |
| `\S`      | 공백 문자 외의 문자               | `[^ \t\n\r\f\v]`     |
| `\w`      | 숫자·알파벳·한글 등을 포함한 문자 | `[가-힣a-zA-Z0-9_]`  |
| `\W`      | 문자 외의 기호                    | `[^가-힣a-zA-Z0-9_]` |



1) search(param)

 : param으로 2개를 넘겨준다. (정규표현식 패턴, 대상), 패턴을 찾으면 match 객체를 반환

 : 해당하는 패턴의 첫 번째 뭉치만 보게 된다.

```python
# re.search(pattern, string) : string에서 pattern과 매치하는 텍스트를 탐색한다.
m = re.search(r'abc', 'abcdefg')
print(m.start())
print(m.end())
print(m.group())

=> 0   # 시작 index
=> 3   # 끝 index(끝은 포함 안됨) 
=> abc # 찾은 string
```

```python
m = re.search('(?<=abc)def', 'abcdef')
```





2) findall

```python
# 가령, <p> 원하는 데이터 </p> 라고 한다면,
re.findall(r'<p>(.?*)</p>', 소스)
```



3) format

 : {인덱스:0개수.자리수f}

```python
'{0:08.2f}'.format(150.37)

=> '00150.37'
```



4) update

 : 딕셔너리 넣을 때 사용한다. 수정, 추가 가능



5) defaultdict 

 : param으로 함수를 넘긴다. 딕셔너리에서 키가 없어도 데이터를 넣을 수 있는게 update였다면, 딕셔너리의 element에 접근할 때, 해당 키가 없어도 null출력하고 키만 넣어준다.

```python
from collections import defaultdict

test_dic = defaultdict(lambda : None)
test_dic.update(a=1)

print(test_dic['b'])
print(test_dic)
```



6) os

 : 파일명을 쉽게!

```python
import os

os.getcwd()          # 현재 위치
os.listdir('datasets/mnist_png/training/*/*.png')  # param 디렉토리에 있는 내용 반환 
```



7) glob

 : 전체 path를 쉽게!

```python
from glob import glob 

data_path = glob('datasets/mnist_png/training/*/*.png')
```



8) tqdm_notebook

 : 반복문 실행상황을 시각적으로 확인할 수 있다.

```python
from tqdm import tqdm_notebook

for path in tqdm_notebook(data_paths):
  /* 어떤 action */
```



9) from __future__ import print_function

 : python2에서 python3처럼 print쓸 수 있게 됨.



10) import argparse

 : 콘솔에서 인자 주기

```python
imoprt argparse

# 객체로 만듦
parser = argparse.ArgumentParser()
# 콘솔에서 py파일 다음에, --src혹은 -s쓰고 뒤에 파라미터 적으면 됨
# help는 해당 소스에 대한 설명이며, required는 아무인자를 주지 않았을때 help를 출력하게 해줌
parser.add_argument('--src', '-s', help='for hyperparam', required=True)
args = parser.parse_args()

print(args.src)
```



11) if __name__ == "__main__":

 : 각 py파일마다 테스트하기 위한 부분으로, 전체 프로젝트 단위로 실행이 될때는 실행되어서는 안되는 코드 영역이다. 

```python
# 실행하는 파일에서 불려지면, 다음 명령어의 출력 값은 main이 되고, import돼서 아래 명령어가 불리면, 해당 모듈명이 출력됨.
print(__name__)
```

